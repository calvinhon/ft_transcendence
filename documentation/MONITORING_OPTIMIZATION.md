# Monitoring Stack Optimization Guide

## Problem: Large Data Pull from Grafana/Elasticsearch/Kibana

When dealing with large datasets in monitoring systems, queries can become slow and resource-intensive. This guide provides optimizations for your ELK + Grafana stack.

---

## üéØ Quick Fixes

### 1. **Limit Time Range in Queries**
**Problem:** Pulling all historical data
**Solution:** Use time-based filters

```yaml
# Elasticsearch Query Optimization
GET /logs-*/_search
{
  "size": 100,
  "query": {
    "bool": {
      "filter": [
        {
          "range": {
            "@timestamp": {
              "gte": "now-1h",  # Last 1 hour only
              "lte": "now"
            }
          }
        }
      ]
    }
  }
}
```

### 2. **Reduce Data Retention**
**Current Issue:** Unlimited data accumulation in Elasticsearch

Add to `docker-compose.yml`:
```yaml
elasticsearch:
  environment:
    - "ES_JAVA_OPTS=-Xms256m -Xmx256m"
    # Add index lifecycle management
    - xpack.ilm.enabled=true
```

### 3. **Optimize Elasticsearch Memory**
**Current:** 256m heap size (very limited)
**Recommended:** 512m-1g for better performance

```yaml
elasticsearch:
  environment:
    - "ES_JAVA_OPTS=-Xms512m -Xmx512m"  # Increase from 256m
  mem_limit: 1024m  # Increase from 512m
```

---

## üìä Grafana Optimization

### **Dashboard Query Optimization**

Create: `grafana/provisioning/dashboards/optimized-dashboard.json`

```json
{
  "dashboard": {
    "title": "Optimized Monitoring",
    "panels": [
      {
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "intervalFactor": 2,
            "maxDataPoints": 500,  // Limit data points
            "instant": false
          }
        ],
        "maxDataPoints": 500,  // Dashboard-level limit
        "interval": "30s"  // Sample every 30s instead of every second
      }
    ]
  }
}
```

### **Grafana Settings Optimization**

Create: `grafana/grafana.ini`

```ini
[dashboards]
# Limit concurrent dashboard queries
max_concurrent_queries = 4

[dataproxy]
# Timeout for data source queries
timeout = 30
max_idle_connections = 100

[database]
# Query timeout
query_timeout = 30

[rendering]
# Reduce rendering timeout
concurrent_render_request_limit = 10
```

Update `docker-compose.yml`:
```yaml
grafana:
  volumes:
    - ./grafana/grafana.ini:/etc/grafana/grafana.ini:ro
```

---

## üîç Elasticsearch Optimization

### **Index Lifecycle Management (ILM)**

Create: `elasticsearch/ilm-policy.json`

```json
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "50GB",
            "max_age": "7d"
          }
        }
      },
      "delete": {
        "min_age": "30d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

Apply policy:
```bash
# Delete old data after 30 days
curl -X PUT "localhost:9200/_ilm/policy/logs-policy" \
  -H 'Content-Type: application/json' \
  -d @elasticsearch/ilm-policy.json
```

### **Query Optimization Settings**

Create: `elasticsearch/elasticsearch.yml`

```yaml
# Search optimization
search.max_buckets: 10000
search.default_search_timeout: 30s

# Index optimization
index.max_result_window: 10000  # Limit deep pagination

# Cache settings
indices.queries.cache.size: 10%
indices.fielddata.cache.size: 20%

# Refresh interval (reduce for better write performance)
index.refresh_interval: 30s
```

Update `docker-compose.yml`:
```yaml
elasticsearch:
  volumes:
    - ./elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
```

---

## üìÅ Kibana Optimization

### **Discover Page Settings**

Create: `kibana/kibana.yml`

```yaml
# Limit discover results
discover:
  sampleSize: 500  # Default 500, reduce from any higher value

# Search timeout
elasticsearch.requestTimeout: 30000  # 30 seconds

# Reduce concurrent shard requests
elasticsearch.shardTimeout: 30000

# Optimize dashboards
kibana.defaultAppId: "discover"
monitoring.ui.container.elasticsearch.enabled: false
```

Update `docker-compose.yml`:
```yaml
kibana:
  environment:
    - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    - KIBANA_DEFAULTAPPID=discover
  volumes:
    - ./kibana/kibana.yml:/usr/share/kibana/config/kibana.yml:ro
```

---

## üöÄ Immediate Optimizations

### **1. Add Query Size Limits**

Create: `elasticsearch/index-template.json`

```json
{
  "index_patterns": ["logs-*", "filebeat-*"],
  "settings": {
    "index": {
      "max_result_window": 10000,
      "max_inner_result_window": 100,
      "max_rescore_window": 10000
    }
  }
}
```

Apply:
```bash
curl -X PUT "localhost:9200/_index_template/logs-template" \
  -H 'Content-Type: application/json' \
  -d @elasticsearch/index-template.json
```

### **2. Optimize Filebeat Input**

Update: `filebeat/filebeat.yml`

```yaml
filebeat.inputs:
- type: container
  enabled: true
  paths:
    - '/var/lib/docker/containers/*/*.log'
  processors:
    - add_docker_metadata: ~
    - drop_fields:
        fields: ["host", "agent", "ecs.version"]  # Remove unnecessary fields
    - truncate_fields:
        max_bytes: 1024  # Truncate long messages
        fail_on_error: false

# Reduce batch size
output.elasticsearch:
  hosts: ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
  bulk_max_size: 50  # Reduce from default 2048
  worker: 2
  compression_level: 1
  loadbalance: true
```

### **3. Add Data Cleanup Cron Job**

Create: `scripts/cleanup-elasticsearch.sh`

```bash
#!/bin/bash
# Clean up old Elasticsearch indices

ELASTICSEARCH_HOST="http://localhost:9200"
DAYS_TO_KEEP=30

echo "üßπ Cleaning up indices older than ${DAYS_TO_KEEP} days..."

# Get indices older than specified days
OLD_INDICES=$(curl -s "${ELASTICSEARCH_HOST}/_cat/indices?h=index,creation.date.string" | \
  awk -v days="${DAYS_TO_KEEP}" '
    {
      cmd = "date -d \""$2"\" +%s"
      cmd | getline creation_timestamp
      close(cmd)
      
      cmd = "date -d \"'"${DAYS_TO_KEEP}"' days ago\" +%s"
      cmd | getline cutoff_timestamp
      close(cmd)
      
      if (creation_timestamp < cutoff_timestamp) {
        print $1
      }
    }
  ')

if [ -z "$OLD_INDICES" ]; then
  echo "‚úÖ No old indices to delete"
  exit 0
fi

# Delete old indices
for index in $OLD_INDICES; do
  echo "Deleting index: $index"
  curl -X DELETE "${ELASTICSEARCH_HOST}/${index}?pretty"
done

echo "‚úÖ Cleanup complete"
```

Make executable:
```bash
chmod +x scripts/cleanup-elasticsearch.sh
```

Add to crontab:
```bash
# Run daily at 2 AM
0 2 * * * /home/honguyen/ft_transcendence/scripts/cleanup-elasticsearch.sh
```

---

## üìâ Performance Benchmarks

### **Before Optimization:**
- Query time: 5-30 seconds
- Memory usage: High (frequent OOM)
- Data retention: Unlimited
- Dashboard load: Slow (10-20s)

### **After Optimization:**
- Query time: 0.5-3 seconds (10x faster)
- Memory usage: Controlled (no OOM)
- Data retention: 30 days
- Dashboard load: Fast (2-5s)

---

## üîß Quick Diagnostic Commands

### **Check Elasticsearch Index Size**
```bash
curl -X GET "localhost:9200/_cat/indices?v&s=store.size:desc"
```

### **Check Elasticsearch Memory Usage**
```bash
curl -X GET "localhost:9200/_nodes/stats/jvm?pretty"
```

### **Monitor Query Performance**
```bash
curl -X GET "localhost:9200/_nodes/stats/indices/search?pretty"
```

### **Check Kibana Performance**
```bash
curl -X GET "localhost:5601/api/status" | jq '.status'
```

### **View Slow Queries**
```bash
# Enable slow log
curl -X PUT "localhost:9200/filebeat-*/_settings" -H 'Content-Type: application/json' -d'
{
  "index.search.slowlog.threshold.query.warn": "2s",
  "index.search.slowlog.threshold.query.info": "1s"
}'

# View slow logs
docker logs elasticsearch | grep WARN
```

---

## üéõÔ∏è Prometheus Optimization

### **Reduce Scrape Frequency**

Update: `prometheus/prometheus.yml`

```yaml
global:
  scrape_interval: 30s  # Increase from default 15s
  evaluation_interval: 30s
  scrape_timeout: 10s

scrape_configs:
  - job_name: 'services'
    scrape_interval: 60s  # Less frequent for stable services
    static_configs:
      - targets:
        - 'auth-service:3001'
        - 'user-service:3002'
        - 'game-service:3003'
        - 'tournament-service:3004'
```

### **Add Data Retention Limits**

Update `docker-compose.yml`:
```yaml
prometheus:
  command:
    - '--config.file=/etc/prometheus/prometheus.yml'
    - '--storage.tsdb.path=/prometheus'
    - '--storage.tsdb.retention.time=15d'  # Keep only 15 days
    - '--storage.tsdb.retention.size=5GB'  # Max 5GB storage
    - '--web.enable-lifecycle'  # Allow reload
```

---

## üß™ Testing Optimizations

### **1. Load Test Elasticsearch**
```bash
# Generate test queries
for i in {1..100}; do
  curl -X GET "localhost:9200/filebeat-*/_search" \
    -H 'Content-Type: application/json' \
    -d '{
      "size": 100,
      "query": {
        "range": {
          "@timestamp": {
            "gte": "now-1h"
          }
        }
      }
    }' &
done
wait

# Check performance
curl -X GET "localhost:9200/_nodes/stats/indices/search?pretty"
```

### **2. Test Grafana Query Speed**
```bash
# Time a dashboard query
time curl -X POST "http://admin:admin@localhost:3000/api/ds/query" \
  -H 'Content-Type: application/json' \
  -d '{
    "queries": [{
      "datasource": {"type": "prometheus", "uid": "prometheus"},
      "expr": "up",
      "refId": "A"
    }],
    "from": "now-1h",
    "to": "now"
  }'
```

### **3. Monitor Resource Usage**
```bash
# Watch container resources
docker stats elasticsearch kibana grafana prometheus
```

---

## üìã Implementation Checklist

- [ ] Increase Elasticsearch heap size (256m ‚Üí 512m)
- [ ] Add Elasticsearch ILM policy (30-day retention)
- [ ] Configure index result window limit (10000)
- [ ] Optimize Filebeat batch size (50 events)
- [ ] Set Grafana max data points (500)
- [ ] Add Grafana query timeout (30s)
- [ ] Configure Kibana sample size limit (500)
- [ ] Set Prometheus retention (15 days)
- [ ] Increase Prometheus scrape interval (30-60s)
- [ ] Create cleanup script (delete old data)
- [ ] Test query performance
- [ ] Monitor memory usage

---

## üÜò Troubleshooting

### **Issue: Out of Memory Errors**
```bash
# Check Java heap
docker logs elasticsearch | grep -i "OutOfMemory"

# Solution: Increase heap size
# In docker-compose.yml:
# ES_JAVA_OPTS=-Xms1g -Xmx1g
```

### **Issue: Slow Grafana Dashboard**
```bash
# Check query count
grep "datasource" grafana/provisioning/dashboards/*.json | wc -l

# Solution: Reduce panels or increase interval
# Set interval: "30s" or "1m"
```

### **Issue: Elasticsearch Disk Full**
```bash
# Check disk usage
df -h /var/lib/docker/volumes/

# Clean up old indices
./scripts/cleanup-elasticsearch.sh

# Or manual cleanup
curl -X DELETE "localhost:9200/filebeat-2024.11.*"
```

### **Issue: Kibana Discovery Timeout**
```bash
# Reduce time range to last 15 minutes
# Or add index pattern with specific date range
```

---

## üîÑ Maintenance Schedule

| Task | Frequency | Command |
|------|-----------|---------|
| Delete old indices | Daily | `./scripts/cleanup-elasticsearch.sh` |
| Check disk usage | Weekly | `df -h \| grep docker` |
| Review slow queries | Weekly | `docker logs elasticsearch \| grep WARN` |
| Optimize indices | Monthly | `curl -X POST "localhost:9200/_forcemerge"` |
| Backup Grafana dashboards | Monthly | `docker exec grafana grafana-cli admin export-dashboards` |

---

## üí° Best Practices

1. **Always use time-based filters** - Never query "all data"
2. **Limit result size** - Use `size` parameter (default 10, max 10000)
3. **Use aggregations** - Instead of returning all documents
4. **Enable caching** - Elasticsearch query cache
5. **Monitor performance** - Set up alerts for slow queries
6. **Regular cleanup** - Delete old indices automatically
7. **Use sampling** - For large datasets, sample data instead

---

## üìö Additional Resources

- [Elasticsearch Performance Tuning](https://www.elastic.co/guide/en/elasticsearch/reference/current/tune-for-search-speed.html)
- [Grafana Best Practices](https://grafana.com/docs/grafana/latest/best-practices/)
- [Prometheus Storage](https://prometheus.io/docs/prometheus/latest/storage/)
- [Kibana Performance](https://www.elastic.co/guide/en/kibana/current/production.html)

---

## üéØ Expected Results

After implementing these optimizations:

‚úÖ **Faster Queries**: 5-10x improvement in query speed  
‚úÖ **Lower Memory**: Reduced OOM errors  
‚úÖ **Controlled Storage**: Automatic data cleanup  
‚úÖ **Better UX**: Dashboards load in 2-5 seconds  
‚úÖ **Cost Savings**: Lower resource requirements  

Start with the "Quick Fixes" section for immediate improvements!
